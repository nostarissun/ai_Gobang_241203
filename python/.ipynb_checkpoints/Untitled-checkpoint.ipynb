{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54b45118-f6a9-426b-9b45-c208d328c482",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1600 and 160000x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 496\u001b[0m\n\u001b[0;32m    489\u001b[0m     model_black \u001b[38;5;241m=\u001b[39m play_with_gambling(model_black, board_size, learning_rate, num_games)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# torch.save(model_black, 'model_black_full.pth')\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m main()\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mself_play\u001b[39m(model_black, model_white, board_size, num_games, learning_rate):\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# chess_board = []\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# for i in range(20):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m#         t.append(0)\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m#     chess_board.append(t)\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03m    执行自我博弈过程并更新模型\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m    model_black: 代表黑棋的模型\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 489\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    485\u001b[0m model_black \u001b[38;5;241m=\u001b[39m GoBang_Model(board_size)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# model_black = torch.load('model_black_full.pth', weights_only = False)\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m model_black \u001b[38;5;241m=\u001b[39m play_with_gambling(model_black, board_size, learning_rate, num_games)\n",
      "Cell \u001b[1;32mIn[51], line 401\u001b[0m, in \u001b[0;36mplay_with_gambling\u001b[1;34m(model, board_size, learning_rate, num_games)\u001b[0m\n\u001b[0;32m    397\u001b[0m rewards_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game_over:\n\u001b[1;32m--> 401\u001b[0m     move \u001b[38;5;241m=\u001b[39m select_move(model, board_state, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m#将每一步棋走动时的棋盘记录下来，并避免使用引用\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     board_states_list\u001b[38;5;241m.\u001b[39mappend(board_state\u001b[38;5;241m.\u001b[39mclone())\n",
      "Cell \u001b[1;32mIn[51], line 189\u001b[0m, in \u001b[0;36mselect_move\u001b[1;34m(model, board_state, temperature)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m根据模型输出的概率分布选择走法\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03mmodel: 五子棋模型\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m:return: 走法坐标\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 189\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(board_state)\n\u001b[0;32m    190\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# 转换为一维概率向量\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (board_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\Gobang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\Gobang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[51], line 173\u001b[0m, in \u001b[0;36mGoBang_Model.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    171\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# print(\"After flatten:\", x.shape)  # 打印展平后的形状\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m    174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(x)\n\u001b[0;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\Gobang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\Gobang\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\Gobang\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1600 and 160000x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "board_size = 20\n",
    "#torch.float32\n",
    "\n",
    "\n",
    "\n",
    "dir = [[0, -1], [1, -1], [1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1]]\n",
    "begin_x = 20\n",
    "begin_y = 20\n",
    "end_x = 590\n",
    "end_y = 590\n",
    "cell_size = 30\n",
    "r = 10\n",
    "status = {\n",
    "    0: 0,\n",
    "    1: 100,\n",
    "    2: 3000,\n",
    "    3: 10000,\n",
    "    4: 20000,\n",
    "    5: 20000\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_evaluate(r : int,  c: int, chess_board):\n",
    "    #分别判断下在此处我方和敌方的得分\n",
    "    #最大化我方得分，抢占敌方最优位置\n",
    "    enemy = judge(r, c, 1, 2, chess_board)\n",
    "    friend = judge(r, c, 2, 1, chess_board)\n",
    "    score = enemy + friend\n",
    "    return score\n",
    "\n",
    "def judge(y0: int, x0: int, enemy: int, friend: int, chess_board):\n",
    "    score = 0\n",
    "    cnt = []\n",
    "\n",
    "    for d in dir:\n",
    "        r = 0\n",
    "        first_empty = -1\n",
    "        for chess_cnt in range(0, 5):\n",
    "            y = y0 + d[1] * chess_cnt\n",
    "            x = x0 + d[0] * chess_cnt\n",
    "            if x >= 20 or x < 0 or y >= 20 or y < 0:\n",
    "                break\n",
    "            if chess_board[y][x] == friend:\n",
    "                break\n",
    "            if chess_board[y][x] == 0 and first_empty == -1:\n",
    "                first_empty = chess_cnt\n",
    "\n",
    "            if chess_board[y][x] == enemy and first_empty <= 2:\n",
    "                r += 1\n",
    "        cnt.append(r)\n",
    "    max_cnt = 0\n",
    "    for i in range(0, 4):\n",
    "        max_cnt = max(max_cnt, cnt[i] + cnt[i + 4])\n",
    "        if max_cnt > 5:\n",
    "            max_cnt = 5\n",
    "    score += status[max_cnt]\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_scores(item1, item2):\n",
    "    \"\"\"\n",
    "    用于比较两个元组中第一个元素（得分）大小的函数\n",
    "    返回值为：\n",
    "    - 若 item1 的得分大于 item2 的得分，返回 1\n",
    "    - 若 item1 的得分小于 item2 的得分，返回 -1\n",
    "    - 若二者得分相等，返回 0\n",
    "    \"\"\"\n",
    "    score1 = item1[0]\n",
    "    score2 = item2[0]\n",
    "    if score1 > score2:\n",
    "        return 1\n",
    "    elif score1 < score2:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def place_where(chess_board):\n",
    "    # scores = []  # 用于存储每个空白位置的得分以及对应的坐标\n",
    "    center_score = 20\n",
    "    res = -1\n",
    "    y = -1\n",
    "    x = -1\n",
    "    for r in range(20):\n",
    "        for c in range(20):\n",
    "            if chess_board[r][c]!= 0:\n",
    "                continue\n",
    "            score = center_score - abs(10 - r) - abs(10 - c) + get_evaluate(r, c, chess_board)\n",
    "            if score > res:\n",
    "                res = score\n",
    "                y = r\n",
    "                x = c\n",
    "            # scores.append((score, (c, r)))  # 将得分和坐标作为元组存入列表\n",
    "    return x, y\n",
    "    # # 使用自定义的比较函数对scores列表进行排序，实现按照得分从高到低排序\n",
    "    # for i in range(len(scores) - 1):\n",
    "    #     for j in range(len(scores) - i - 1):\n",
    "    #         if compare_scores(scores[j], scores[j + 1]) < 0:\n",
    "    #             scores[j], scores[j + 1] = scores[j + 1], scores[j]\n",
    "\n",
    "    # ten_scores = scores[:3]  # 取前十个得分最高的位置（如果不足十个则取全部）\n",
    "    # if len(ten_scores) == 0:\n",
    "    #     return []  # 如果没有可落子的空白位置，返回空列表\n",
    "\n",
    "    # choice = random.randint(0, len(ten_scores) - 1)  # 随机选择一个索引\n",
    "    # return list(ten_scores[choice][1])  # 返回选中位置的坐标（转换为列表形式）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GoBang_Model(nn.Module):\n",
    "\n",
    "    #模型初始化\n",
    "    def __init__(self, board_size):\n",
    "        super(GoBang_Model, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        #第一个卷积层输入1通道，输出32通道，卷积核大小为3，填充大小1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        #第一次增加非线性因素\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #第一个池化层使用2x2的池化核，步长为2\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        #第二个卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        #第二次增加非线性因素\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #第二个池化层\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        #第一个全连接层\n",
    "        #输出特征数量设置为 128，两个池化层//4\n",
    "        # print(board_size)\n",
    "        # print(\"fc1:\", 64 * (board_size // 4) * (board_size // 4))\n",
    "        self.fc1 = nn.Linear(64 * (board_size // 4) * (board_size // 4), 128)\n",
    "        #第三次引入非线性因素\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #第二个全连接层\n",
    "        self.fc2 = nn.Linear(128, board_size * board_size)\n",
    "\n",
    "    #前向传播\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape:\", x.shape)  # 打印输入形状\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After conv1:\", x.shape)  # 打印第一个卷积层后的形状\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        # print(\"After pool1:\", x.shape)  # 打印第一个池化层后的形状\n",
    "        x = self.conv2(x)\n",
    "        # print(\"After conv2:\", x.shape)  # 打印第二个卷积层后的形状\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # print(\"After pool2:\", x.shape)  # 打印第二个池化层后的形状\n",
    "        x = x.view(1, -1)\n",
    "        # print(\"After flatten:\", x.shape)  # 打印展平后的形状\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def select_move(model, board_state, temperature=1.0):\n",
    "    \"\"\"\n",
    "    根据模型输出的概率分布选择走法\n",
    "    model: 五子棋模型\n",
    "    board_state: 当前棋盘状态张量，形状为(1, board_size, board_size)\n",
    "    temperature: 温度参数，用于控制采样的随机性，温度越高越随机，越低越偏向确定性选择\n",
    "    :return: 走法坐标\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(board_state)\n",
    "        output = output.squeeze(0).flatten()  # 转换为一维概率向量\n",
    "\n",
    "        mask = (board_state.squeeze(0) == 0).flatten().float()\n",
    "        output = output * mask  # 将已下棋的位置的得分设为0\n",
    "        \n",
    "        # 根据温度参数调整概率分布\n",
    "        # 根据 temperature 参数的值来分情况处理走法选择的逻辑。temperature 参数用于控制选择走法的随机性程度，\n",
    "        # 当它大于 0 时，采用一种基于概率采样的方式来选择走法，使得选择具有一定随机性，更有利于探索不同的走法可能性，\n",
    "        # 尤其在训练早期探索更多不同走法对模型学习更全面的策略有帮助。\n",
    "        if temperature > 0:\n",
    "            # 在这里，先将 output 除以 temperature，temperature 的作用类似一个缩放因子，\n",
    "            # 温度越高，经过 softmax 后得到的概率分布越 “平缓”，意味着各个位置被选中的概率相对更均匀，随机性就越大；温度越低，概率分布越 “尖锐”，\n",
    "            # 模型认为最优的几个位置的概率就会占比极大，随机性就越小，更偏向于确定性地选择模型认为最好的走法。\n",
    "            probs = torch.softmax(output / temperature, dim=0).cpu().numpy()\n",
    "            move_index = np.random.choice(len(probs), p=probs)\n",
    "        else:\n",
    "            move_index = torch.argmax(output).item()\n",
    "\n",
    "        row = move_index // model.board_size\n",
    "        col = move_index % model.board_size\n",
    "\n",
    "    return row, col\n",
    "    \n",
    "def update_board_state(board_state, move, chess):\n",
    "    \"\"\"\n",
    "    根据走法更新棋盘状态张量\n",
    "    param board_state: 当前棋盘状态张量\n",
    "    param move: 走法坐标（行，列）\n",
    "    chess: 棋子颜色（如1表示黑棋，2表示白棋）\n",
    "    return: 更新后的棋盘状态张量\n",
    "    \"\"\"\n",
    "    row, col = move\n",
    "    board_state[0][row][col] = chess\n",
    "    return board_state\n",
    "    \n",
    "\n",
    "def is_game_over(board_state, board_size):\n",
    "    \"\"\"\n",
    "    判断五子棋棋局是否结束\n",
    "    :param board_state: 棋盘状态张量\n",
    "    :param board_size: 棋盘大小\n",
    "    :return: 是否结束（True/False）以及获胜方（None表示未结束或平局，1表示黑棋胜，2表示白棋胜）\n",
    "    \"\"\"\n",
    "    directions = [(1, 0), (-1, 0), (0, 1), (0, -1), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
    "    for row in range(board_size):\n",
    "        for col in range(board_size):\n",
    "            if board_state[0][row][col]!= 0:\n",
    "                piece_color = board_state[0][row][col]\n",
    "                for direction in directions:\n",
    "                    count = 1\n",
    "                    for step in range(1, 5):\n",
    "                        new_row = row + step * direction[0]\n",
    "                        new_col = col + step * direction[1]\n",
    "                        if 0 <= new_row < board_size and 0 <= new_col < board_size and board_state[0][new_row][new_col] == piece_color:\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    if count >= 5:\n",
    "                        return True, piece_color\n",
    "\n",
    "    if (board_state == 0).sum() == 0:  # 棋盘已满判断平局\n",
    "        return True, None\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def calculate_reward(result, chess):\n",
    "    \"\"\"\n",
    "    根据棋局结果和执子方计算奖励\n",
    "    result: 棋局结果（None表示未结束或平局，1表示黑棋胜，2表示白棋胜）\n",
    "    piece_color: 执子方棋子颜色（如1表示黑棋，2表示白棋）\n",
    "    return: 奖励值\n",
    "    \"\"\"\n",
    "    if result == chess:\n",
    "        return 1 \n",
    "    elif result is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def count_connectivity(board_state, piece_color):\n",
    "    \"\"\"\n",
    "    计算给定棋盘状态下指定颜色棋子的连通情况\n",
    "    :param board_state: 棋盘状态张量，形状如(1, board_size, board_size)\n",
    "    :param piece_color: 棋子颜色（如示例中用1表示己方棋子）\n",
    "    :return: 连通数量或者连通程度的某种量化表示（例如连通子图的数量、最长连通长度等，根据需求定义）\n",
    "    \"\"\"\n",
    "    board_size = board_state.shape[1]\n",
    "    connectivity_count = 0\n",
    "    # 可以通过遍历棋盘每个位置来检查棋子连通性\n",
    "    for row in range(board_size):\n",
    "        for col in range(board_size):\n",
    "            if board_state[0][row][col] == piece_color:\n",
    "                # 这里可以进一步编写代码来检查该棋子与周围同色棋子的连通情况\n",
    "                # 比如向上下左右、斜向等方向遍历查找相连的同色棋子，统计连通数量等\n",
    "                # 以下是简单示意，假设发现相连就增加连通计数（实际需要更严谨逻辑）\n",
    "                connectivity_count += 1\n",
    "    return connectivity_count\n",
    "\n",
    "\n",
    "def update_model(model, optimizer, board_states, actions, rewards):\n",
    "    \"\"\"\n",
    "    根据奖励更新模型参数\n",
    "    model: 五子棋模型\n",
    "    optimizer: 优化器（如Adam等）\n",
    "    board_states: 整个棋局过程中的棋盘状态张量列表\n",
    "    actions: 整个棋局过程中的走法坐标列表\n",
    "    rewards: 对应的奖励值列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 总损失初始化\n",
    "    total_loss = 0\n",
    "    # 用于存储每步的基础均方误差损失（MSE），方便后续归一化及与其他损失项结合\n",
    "    mse_losses = []\n",
    "    # 用于存储每步的策略相关损失（例如基于走法概率分布的损失），方便后续处理\n",
    "    policy_losses = []\n",
    "    # 用于存储每步的基于五子棋规则特性的损失，引导模型学习符合规则的策略\n",
    "    rule_based_losses = []\n",
    "\n",
    "    for board_state, action, reward in zip(board_states, actions, rewards):\n",
    "        output = model(board_state)\n",
    "        row, col = action\n",
    "        action_index = row * model.board_size + col\n",
    "\n",
    "        # 构建目标张量\n",
    "        target = torch.zeros_like(output).squeeze(0).flatten()\n",
    "        target[action_index] = reward\n",
    "\n",
    "        # 1. 计算基础均方误差损失（MSE）\n",
    "        mse_loss = ((output.squeeze(0).flatten() - target) ** 2).sum()\n",
    "        mse_losses.append(mse_loss)\n",
    "\n",
    "        # 2. 计算策略相关损失（示例采用负对数似然损失来优化走法概率分布）\n",
    "        probs = torch.softmax(output.squeeze(0).flatten(), dim=0)\n",
    "        nll_loss = -torch.log(probs[action_index])\n",
    "        policy_losses.append(nll_loss)\n",
    "\n",
    "        # 3. 计算基于五子棋规则特性的损失\n",
    "        # 这里简单示例，判断走法是否破坏自身连珠优势（可根据实际深入扩展和细化规则）\n",
    "        # 获取当前棋盘上自身棋子的连通情况（需要定义相应函数来分析棋盘连通性，比如count_connectivity函数）\n",
    "        own_connectivity = count_connectivity(board_state, 1)  # 假设1表示己方棋子\n",
    "        new_board_state = update_board_state(board_state.clone(), action, 1)  # 模拟走这步后的棋盘\n",
    "        new_connectivity = count_connectivity(new_board_state, 1)\n",
    "        if new_connectivity < own_connectivity:\n",
    "            rule_loss = torch.tensor(0.1, requires_grad=True)  # 若破坏连珠优势，给予一定损失惩罚\n",
    "        else:\n",
    "            rule_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        rule_based_losses.append(rule_loss)\n",
    "\n",
    "    # 对每步的MSE损失进行归一化处理\n",
    "    if mse_losses:\n",
    "        mean_mse_loss = torch.stack(mse_losses).mean()\n",
    "    else:\n",
    "        mean_mse_loss = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # 对每步的策略相关损失进行归一化处理\n",
    "    if policy_losses:\n",
    "        mean_policy_loss = torch.stack(policy_losses).mean()\n",
    "    else:\n",
    "        mean_policy_loss = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # 对每步的规则特性损失进行归一化处理\n",
    "    if rule_based_losses:\n",
    "        mean_rule_loss = torch.stack(rule_based_losses).mean()\n",
    "    else:\n",
    "        mean_rule_loss = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "    # 组合不同类型的损失，可根据实际情况调整权重\n",
    "    total_loss = mean_mse_loss + mean_policy_loss + mean_rule_loss\n",
    "\n",
    "    print(total_loss)\n",
    "    if total_loss == 0:\n",
    "        return\n",
    "\n",
    "    # 反向传播\n",
    "    total_loss.backward()\n",
    "\n",
    "    # 学习率调整（示例采用简单的固定步长衰减，可替换为更复杂的策略如余弦退火等）\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= 0.99  # 每次训练步长衰减学习率，可根据实际调整衰减因子\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "#白棋\n",
    "def play_with_gambling(model, board_size, learning_rate, num_games):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "\n",
    "        chess_board = []\n",
    "        for i in range(20):\n",
    "            t = []\n",
    "            for j in range(20):\n",
    "                t.append(0)\n",
    "            chess_board.append(t)\n",
    "            \n",
    "        \n",
    "        board_state = torch.tensor(chess_board).reshape(1, 20, 20).float()\n",
    "        game_over = False\n",
    "        board_states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "\n",
    "        while not game_over:\n",
    "            \n",
    "            move = select_move(model, board_state, 0.2)\n",
    "            #将每一步棋走动时的棋盘记录下来，并避免使用引用\n",
    "            board_states_list.append(board_state.clone())\n",
    "            #记录每一步的走法\n",
    "            actions_list.append(move)            \n",
    "            board_state = update_board_state(board_state, move, 1)\n",
    "            chess_board[move[0]][move[1]] = 1\n",
    "\n",
    "            game_over, result = is_game_over(board_state, board_size)\n",
    "\n",
    "            \n",
    "            if not game_over:\n",
    "                                #确定走法\n",
    "\n",
    "                x, y = place_where(chess_board)\n",
    "                board_states_list.append(board_state.clone())\n",
    "                actions_list.append([y, x])\n",
    "                board_state = update_board_state(board_state, [y, x], 2)\n",
    "                chess_board[y][x] = 2\n",
    "    \n",
    "                game_over, result = is_game_over(board_state, board_size)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            #一局结束分配奖励\n",
    "            if game_over:\n",
    "                reward = calculate_reward(result, 1)\n",
    "                rewards_list.append(reward)\n",
    "                update_model(model, optimizer, board_states_list, actions_list, rewards_list)\n",
    "                # records.append(record)\n",
    "                if result == 1:\n",
    "                    print(\"博弈输\")\n",
    "                    for i in range(20):\n",
    "                        for j in range(20):\n",
    "                            if chess_board[i][j] == 0:\n",
    "                                print(\".\", end = ' ')\n",
    "                            else:\n",
    "                                print(chess_board[i][j], end = ' ')\n",
    "                        print()\n",
    "                elif result == 2:\n",
    "                    print(\"博弈胜\")\n",
    "                    for i in range(20):\n",
    "                        for j in range(20):\n",
    "                            if chess_board[i][j] == 0:\n",
    "                                print(\".\", end = ' ')\n",
    "                            else:\n",
    "                                print(chess_board[i][j], end = ' ')\n",
    "                        print()\n",
    "                else:\n",
    "                    print(\"平局\")\n",
    "                               \n",
    "    print(\"over\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# def dynamic_loss(record_list, line, ax):\n",
    "#     x_data = list(range(len(record_list)))\n",
    "#     # 确保record_list里的Tensor元素都经过detach处理，转为普通数值类型（比如float）\n",
    "#     y_data = [record.detach().item() if isinstance(record, torch.Tensor) else float(record) if isinstance(record, (int, float)) else 0 for record in record_list]\n",
    "#     line.set_data(x_data, y_data)\n",
    "#     ax.relim()\n",
    "#     ax.autoscale_view()\n",
    "#     plt.draw()\n",
    "#     plt.pause(0.01)\n",
    "#     clear_output(wait=True)\n",
    "#     time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    board_size = 200\n",
    "    num_games = 100\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    \n",
    "    # model_white = GoBang_Model(board_size)\n",
    "    model_black = GoBang_Model(board_size)\n",
    "    \n",
    "    # model_black = torch.load('model_black_full.pth', weights_only = False)\n",
    "    \n",
    "    model_black = play_with_gambling(model_black, board_size, learning_rate, num_games)\n",
    "\n",
    "\n",
    "\n",
    "    # torch.save(model_black, 'model_black_full.pth')\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "def self_play(model_black, model_white, board_size, num_games, learning_rate):\n",
    "    # chess_board = []\n",
    "    # for i in range(20):\n",
    "    #     t = []\n",
    "    #     for j in range(20):\n",
    "    #         t.append(0)\n",
    "    #     chess_board.append(t)\n",
    "    \"\"\"\n",
    "    执行自我博弈过程并更新模型\n",
    "    model_black: 代表黑棋的模型\n",
    "    model_white: 代表白棋的模型\n",
    "    board_size: 棋盘大小\n",
    "    num_games: 自我博弈的局数\n",
    "    learning_rate: 学习率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #创建 Adam 优化器\n",
    "    #将模型中所有可学习的参数传递给优化器\n",
    "    optimizer_black = optim.Adam(model_black.parameters(), lr=learning_rate)\n",
    "    optimizer_white = optim.Adam(model_white.parameters(), lr=learning_rate)\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        board_state = torch.zeros(1, 20, 20).float()\n",
    "        game_over = False\n",
    "        current_color = 1  # 黑棋先下，用1表示\n",
    "        board_states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "\n",
    "        while not game_over:\n",
    "\n",
    "            #选择下旗方\n",
    "            if current_color == 1:\n",
    "                model = model_black\n",
    "            else:\n",
    "                model = model_white\n",
    "\n",
    "            #确定走法\n",
    "            move = select_move(model, board_state)\n",
    "            #将每一步棋走动时的棋盘记录下来，并避免使用引用\n",
    "            board_states_list.append(board_state.clone())\n",
    "            #记录每一步的走法\n",
    "            actions_list.append(move)\n",
    "\n",
    "            board_state = update_board_state(board_state, move, current_color)\n",
    "            game_over, result = is_game_over(board_state, board_size)\n",
    "\n",
    "\n",
    "            #一局结束分配奖励\n",
    "            if game_over:\n",
    "                reward = calculate_reward(result, current_color, 1)\n",
    "                rewards_list.append(reward)\n",
    "                if result == 1:\n",
    "                    update_model(model_black, optimizer_black, board_states_list, actions_list, rewards_list)\n",
    "                    print(\"黑胜\")\n",
    "                elif result == 2:\n",
    "                    update_model(model_white, optimizer_white, board_states_list, actions_list, rewards_list)\n",
    "                    print(\"白胜\")\n",
    "                else:\n",
    "                    print(\"平局\")\n",
    "            else:\n",
    "                current_color = 3 - current_color  # 切换棋子颜色\n",
    "\n",
    "    return model_black, model_white\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3451899f-7bcb-4833-8912-1411a8e08000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\联想\\AppData\\Local\\Temp\\ipykernel_48060\\1032313861.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_black = torch.load('model_black_full.pth')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_white' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# 将加载的权重数据应用到模型实例上\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     handle_connection(model_white)\n\u001b[1;32m--> 165\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[41], line 163\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m model_black \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_black_full.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# 将加载的权重数据应用到模型实例上\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m handle_connection(model_white)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_white' is not defined"
     ]
    }
   ],
   "source": [
    "import const\n",
    "import socket\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "chess_board = []\n",
    "for init_chess_board in range(const.board_size_H):\n",
    "    t = []\n",
    "    for j in range(const.board_size_W):\n",
    "        t.append(0)\n",
    "    chess_board.append(t)\n",
    "\n",
    "\n",
    "class GoBang_Model(nn.Module):\n",
    "\n",
    "    #模型初始化\n",
    "    def __init__(self, board_size):\n",
    "        super(GoBang_Model, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        #第一个卷积层输入1通道，输出32通道，卷积核大小为3，填充大小1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        #第一次增加非线性因素\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #第一个池化层使用2x2的池化核，步长为2\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        #第二个卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        #第二次增加非线性因素\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #第二个池化层\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        #第一个全连接层\n",
    "        #输出特征数量设置为 128，两个池化层//4\n",
    "        # print(board_size)\n",
    "        # print(\"fc1:\", 64 * (board_size // 4) * (board_size // 4))\n",
    "        self.fc1 = nn.Linear(64 * (board_size // 4) * (board_size // 4), 128)\n",
    "        #第三次引入非线性因素\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #第二个全连接层\n",
    "        self.fc2 = nn.Linear(128, board_size * board_size)\n",
    "\n",
    "    #前向传播\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape:\", x.shape)  # 打印输入形状\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After conv1:\", x.shape)  # 打印第一个卷积层后的形状\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        # print(\"After pool1:\", x.shape)  # 打印第一个池化层后的形状\n",
    "        x = self.conv2(x)\n",
    "        # print(\"After conv2:\", x.shape)  # 打印第二个卷积层后的形状\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # print(\"After pool2:\", x.shape)  # 打印第二个池化层后的形状\n",
    "        x = x.view(1, -1)\n",
    "        # print(\"After flatten:\", x.shape)  # 打印展平后的形状\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def select_move(model, board_state, temperature=1.0):\n",
    "    \"\"\"\n",
    "    根据模型输出的概率分布选择走法\n",
    "    model: 五子棋模型\n",
    "    board_state: 当前棋盘状态张量，形状为(1, board_size, board_size)\n",
    "    temperature: 温度参数，用于控制采样的随机性，温度越高越随机，越低越偏向确定性选择\n",
    "    :return: 走法坐标\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(board_state)\n",
    "        output = output.squeeze(0).flatten()  # 转换为一维概率向量\n",
    "        \n",
    "        mask = (board_state.squeeze(0) == 0).flatten().float()\n",
    "        output = output * mask  # 将已下棋的位置的得分设为0\n",
    "        \n",
    "        # 根据温度参数调整概率分布\n",
    "        # 根据 temperature 参数的值来分情况处理走法选择的逻辑。temperature 参数用于控制选择走法的随机性程度，\n",
    "        # 当它大于 0 时，采用一种基于概率采样的方式来选择走法，使得选择具有一定随机性，更有利于探索不同的走法可能性，\n",
    "        # 尤其在训练早期探索更多不同走法对模型学习更全面的策略有帮助。\n",
    "        if temperature > 0:\n",
    "            # 在这里，先将 output 除以 temperature，temperature 的作用类似一个缩放因子，\n",
    "            # 温度越高，经过 softmax 后得到的概率分布越 “平缓”，意味着各个位置被选中的概率相对更均匀，随机性就越大；温度越低，概率分布越 “尖锐”，\n",
    "            # 模型认为最优的几个位置的概率就会占比极大，随机性就越小，更偏向于确定性地选择模型认为最好的走法。\n",
    "            probs = torch.softmax(output / temperature, dim=0).cpu().numpy()\n",
    "            move_index = np.random.choice(len(probs), p=probs)\n",
    "        else:\n",
    "            move_index = torch.argmax(output).item()\n",
    "\n",
    "        row = move_index // model.board_size\n",
    "        col = move_index % model.board_size\n",
    "\n",
    "    return col, row\n",
    "\n",
    "\n",
    "def handle_connection(model):\n",
    "    global chess_board\n",
    "    serveport = 12000\n",
    "  \n",
    "    servesocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "    servesocket.bind(('', serveport))\n",
    "    servesocket.listen(1)\n",
    "    print(\"运行中...\\n\")\n",
    "    cnt = 1\n",
    "    while True:\n",
    "        connectionsocket, addr = servesocket.accept()\n",
    "        sentence = connectionsocket.recv(1024).decode()\n",
    "\n",
    "        # 检查接收到的数据长度是否符合预期\n",
    "        length = const.board_size_W * const.board_size_H\n",
    "        if len(sentence) != length:\n",
    "            print(\"接收到的数据长度不正确，请重新发送\" + str(len(sentence)))\n",
    "            print(sentence)\n",
    "            # connectionsocket.send(error_message.encode())\n",
    "            # connectionsocket.close()\n",
    "            continue\n",
    "\n",
    "\n",
    "        for i in range(const.board_size_H):\n",
    "            for j in range(const.board_size_W):\n",
    "                chess_board[i][j] = (int(sentence[const.board_size_W * i + j]))\n",
    "                print(chess_board[i][j], end='')\n",
    "            print()\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "\n",
    "\n",
    "        board_state = torch.tensor(chess_board).reshape(1, 20, 20).float()\n",
    "\n",
    "        #先发y再发x\n",
    "        num = select_move(model, board_state, 1)\n",
    "        print(num)\n",
    "        s = str(num[1]) + \",\" + str(num[0])\n",
    "        # print(s)\n",
    "\n",
    "        print(\"已处理\" + str(cnt) + \"条\\n\")\n",
    "        cnt += 1\n",
    "\n",
    "        if cnt == 10:\n",
    "            break\n",
    "\n",
    "\n",
    "        \n",
    "        if cnt > 999 :\n",
    "            cnt = 999\n",
    "\n",
    "        connectionsocket.send(s.encode())\n",
    "        connectionsocket.close()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # model = torch.load('model_white_full.pth')\n",
    "    # model_white = GoBang_Model(board_size)\n",
    "    # state_dict_white = torch.load('model_white_full.pth')\n",
    "    # model_white.load_state_dict(state_dict_white)\n",
    "    model_black = torch.load('model_black_full.pth')\n",
    "    # 将加载的权重数据应用到模型实例上\n",
    "    handle_connection(model_white)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbeb70b-b472-4d4e-abad-9e8d626ab3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "board_size = 20\n",
    "#torch.float32\n",
    "\n",
    "\n",
    "\n",
    "dir = [[0, -1], [1, -1], [1, 0], [1, 1], [0, 1], [-1, 1], [-1, 0], [-1, -1]]\n",
    "begin_x = 20\n",
    "begin_y = 20\n",
    "end_x = 590\n",
    "end_y = 590\n",
    "cell_size = 30\n",
    "r = 10\n",
    "status = {\n",
    "    0: 0,\n",
    "    1: 100,\n",
    "    2: 3000,\n",
    "    3: 10000,\n",
    "    4: 20000,\n",
    "    5: 20000\n",
    "}\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_evaluate(r : int,  c: int, chess_board):\n",
    "    #分别判断下在此处我方和敌方的得分\n",
    "    #最大化我方得分，抢占敌方最优位置\n",
    "    enemy = judge(r, c, 1, 2, chess_board)\n",
    "    friend = judge(r, c, 2, 1, chess_board)\n",
    "    score = enemy + friend\n",
    "    return score\n",
    "\n",
    "def judge(y0: int, x0: int, enemy: int, friend: int, chess_board):\n",
    "    score = 0\n",
    "    cnt = []\n",
    "\n",
    "    for d in dir:\n",
    "        r = 0\n",
    "        first_empty = -1\n",
    "        for chess_cnt in range(0, 5):\n",
    "            y = y0 + d[1] * chess_cnt\n",
    "            x = x0 + d[0] * chess_cnt\n",
    "            if x >= 20 or x < 0 or y >= 20 or y < 0:\n",
    "                break\n",
    "            if chess_board[y][x] == friend:\n",
    "                break\n",
    "            if chess_board[y][x] == 0 and first_empty == -1:\n",
    "                first_empty = chess_cnt\n",
    "\n",
    "            if chess_board[y][x] == enemy and first_empty <= 2:\n",
    "                r += 1\n",
    "        cnt.append(r)\n",
    "    max_cnt = 0\n",
    "    for i in range(0, 4):\n",
    "        max_cnt = max(max_cnt, cnt[i] + cnt[i + 4])\n",
    "        if max_cnt > 5:\n",
    "            max_cnt = 5;\n",
    "    score += status[max_cnt]\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_scores(item1, item2):\n",
    "    \"\"\"\n",
    "    用于比较两个元组中第一个元素（得分）大小的函数\n",
    "    返回值为：\n",
    "    - 若 item1 的得分大于 item2 的得分，返回 1\n",
    "    - 若 item1 的得分小于 item2 的得分，返回 -1\n",
    "    - 若二者得分相等，返回 0\n",
    "    \"\"\"\n",
    "    score1 = item1[0]\n",
    "    score2 = item2[0]\n",
    "    if score1 > score2:\n",
    "        return 1\n",
    "    elif score1 < score2:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def place_where(chess_board):\n",
    "    scores = []  # 用于存储每个空白位置的得分以及对应的坐标\n",
    "    center_score = 20\n",
    "\n",
    "    for r in range(20):\n",
    "        for c in range(20):\n",
    "            if chess_board[r][c]!= 0:\n",
    "                continue\n",
    "            score = center_score - abs(10 - r) - abs(10 - c) + get_evaluate(r, c, chess_board)\n",
    "            scores.append((score, (c, r)))  # 将得分和坐标作为元组存入列表\n",
    "\n",
    "    # 使用自定义的比较函数对scores列表进行排序，实现按照得分从高到低排序\n",
    "    for i in range(len(scores) - 1):\n",
    "        for j in range(len(scores) - i - 1):\n",
    "            if compare_scores(scores[j], scores[j + 1]) < 0:\n",
    "                scores[j], scores[j + 1] = scores[j + 1], scores[j]\n",
    "\n",
    "    ten_scores = scores[:3]  # 取前十个得分最高的位置（如果不足十个则取全部）\n",
    "    if len(ten_scores) == 0:\n",
    "        return []  # 如果没有可落子的空白位置，返回空列表\n",
    "\n",
    "    choice = random.randint(0, len(ten_scores) - 1)  # 随机选择一个索引\n",
    "    return list(ten_scores[choice][1])  # 返回选中位置的坐标（转换为列表形式）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GoBang_Model(nn.Module):\n",
    "\n",
    "    #模型初始化\n",
    "    def __init__(self, board_size):\n",
    "        super(GoBang_Model, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        #第一个卷积层输入1通道，输出32通道，卷积核大小为3，填充大小1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        #第一次增加非线性因素\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #第一个池化层使用2x2的池化核，步长为2\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        #第二个卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        #第二次增加非线性因素\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #第二个池化层\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        #第一个全连接层\n",
    "        #输出特征数量设置为 128，两个池化层//4\n",
    "        # print(board_size)\n",
    "        # print(\"fc1:\", 64 * (board_size // 4) * (board_size // 4))\n",
    "        self.fc1 = nn.Linear(64 * (board_size // 4) * (board_size // 4), 128)\n",
    "        #第三次引入非线性因素\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #第二个全连接层\n",
    "        self.fc2 = nn.Linear(128, board_size * board_size)\n",
    "\n",
    "    #前向传播\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape:\", x.shape)  # 打印输入形状\n",
    "        x = self.conv1(x)\n",
    "        # print(\"After conv1:\", x.shape)  # 打印第一个卷积层后的形状\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        # print(\"After pool1:\", x.shape)  # 打印第一个池化层后的形状\n",
    "        x = self.conv2(x)\n",
    "        # print(\"After conv2:\", x.shape)  # 打印第二个卷积层后的形状\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # print(\"After pool2:\", x.shape)  # 打印第二个池化层后的形状\n",
    "        x = x.view(1, -1)\n",
    "        # print(\"After flatten:\", x.shape)  # 打印展平后的形状\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def select_move(model, board_state, temperature=1.0):\n",
    "    \"\"\"\n",
    "    根据模型输出的概率分布选择走法\n",
    "    model: 五子棋模型\n",
    "    board_state: 当前棋盘状态张量，形状为(1, board_size, board_size)\n",
    "    temperature: 温度参数，用于控制采样的随机性，温度越高越随机，越低越偏向确定性选择\n",
    "    :return: 走法坐标\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(board_state)\n",
    "        output = output.squeeze(0).flatten()  # 转换为一维概率向量\n",
    "\n",
    "        # 根据温度参数调整概率分布\n",
    "        # 根据 temperature 参数的值来分情况处理走法选择的逻辑。temperature 参数用于控制选择走法的随机性程度，\n",
    "        # 当它大于 0 时，采用一种基于概率采样的方式来选择走法，使得选择具有一定随机性，更有利于探索不同的走法可能性，\n",
    "        # 尤其在训练早期探索更多不同走法对模型学习更全面的策略有帮助。\n",
    "        if temperature > 0:\n",
    "            # 在这里，先将 output 除以 temperature，temperature 的作用类似一个缩放因子，\n",
    "            # 温度越高，经过 softmax 后得到的概率分布越 “平缓”，意味着各个位置被选中的概率相对更均匀，随机性就越大；温度越低，概率分布越 “尖锐”，\n",
    "            # 模型认为最优的几个位置的概率就会占比极大，随机性就越小，更偏向于确定性地选择模型认为最好的走法。\n",
    "            probs = torch.softmax(output / temperature, dim=0)\n",
    "            #使用pytorch的随机数，避免传回cpu\n",
    "            move_index = torch.multinomial(probs, 1).item()\n",
    "        else:\n",
    "            move_index = torch.argmax(output).item()\n",
    "\n",
    "        row = move_index // model.board_size\n",
    "        col = move_index % model.board_size\n",
    "\n",
    "    return row, col\n",
    "    \n",
    "def update_board_state(board_state, move, chess):\n",
    "    \"\"\"\n",
    "    根据走法更新棋盘状态张量\n",
    "    param board_state: 当前棋盘状态张量\n",
    "    param move: 走法坐标（行，列）\n",
    "    chess: 棋子颜色（如1表示黑棋，2表示白棋）\n",
    "    return: 更新后的棋盘状态张量\n",
    "    \"\"\"\n",
    "    row, col = move\n",
    "    board_state[0][row][col] = chess\n",
    "    return board_state\n",
    "    \n",
    "\n",
    "def is_game_over(board_state, board_size):\n",
    "    \"\"\"\n",
    "    判断五子棋棋局是否结束\n",
    "    :param board_state: 棋盘状态张量\n",
    "    :param board_size: 棋盘大小\n",
    "    :return: 是否结束（True/False）以及获胜方（None表示未结束或平局，1表示黑棋胜，2表示白棋胜）\n",
    "    \"\"\"\n",
    "    directions = [(1, 0), (-1, 0), (0, 1), (0, -1), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
    "    for row in range(board_size):\n",
    "        for col in range(board_size):\n",
    "            if board_state[0][row][col]!= 0:\n",
    "                piece_color = board_state[0][row][col]\n",
    "                for direction in directions:\n",
    "                    count = 1\n",
    "                    for step in range(1, 5):\n",
    "                        new_row = row + step * direction[0]\n",
    "                        new_col = col + step * direction[1]\n",
    "                        if 0 <= new_row < board_size and 0 <= new_col < board_size and board_state[0][new_row][new_col] == piece_color:\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    if count >= 5:\n",
    "                        return True, piece_color\n",
    "\n",
    "    if (board_state == 0).sum() == 0:  # 棋盘已满判断平局\n",
    "        return True, None\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def calculate_reward(result, chess, x):\n",
    "    \"\"\"\n",
    "    根据棋局结果和执子方计算奖励\n",
    "    result: 棋局结果（None表示未结束或平局，1表示黑棋胜，2表示白棋胜）\n",
    "    piece_color: 执子方棋子颜色（如1表示黑棋，2表示白棋）\n",
    "    return: 奖励值\n",
    "    \"\"\"\n",
    "    if result == chess:\n",
    "        return 1 * x\n",
    "    elif result is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 * x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_model(model, optimizer, board_states, actions, rewards):\n",
    "    \"\"\"\n",
    "    根据奖励更新模型参数\n",
    "    model: 五子棋模型\n",
    "    optimizer: 优化器（如Adam等）\n",
    "    board_states: 整个棋局过程中的棋盘状态张量列表\n",
    "    actions: 整个棋局过程中的走法坐标列表\n",
    "    rewards: 对应的奖励值列表\n",
    "\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    # 将board_states列表转换为张量并移动到GPU\n",
    "    board_states_tensor = torch.stack(board_states).to(device)\n",
    "    # 将actions列表转换为张量并移动到GPU，\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    # 将rewards列表转换为张量并移动到GPU\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(board_states)):\n",
    "        board_state = board_states_tensor[i].unsqueeze(0)\n",
    "        action = actions_tensor[i].unsqueeze(0)\n",
    "        reward = rewards_tensor[0]\n",
    "        output = model(board_state)\n",
    "        row, col = action[:, 0], action[:, 1]\n",
    "        action_index = row * model.board_size + col\n",
    "        target = torch.zeros_like(output).squeeze(0).flatten()\n",
    "        target[action_index] = reward\n",
    "        loss += ((output.squeeze(0).flatten() - target) ** 2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# def update_model(model, optimizer, board_states, actions, rewards):\n",
    "#     \"\"\"\n",
    "#     根据奖励更新模型参数\n",
    "#     model: 五子棋模型\n",
    "#     optimizer: 优化器（如Adam等）\n",
    "#     board_states: 整个棋局过程中的棋盘状态张量列表\n",
    "#     actions: 整个棋局过程中的走法坐标列表\n",
    "#     rewards: 对应的奖励值列表\n",
    "\n",
    "#     \"\"\"\n",
    "#     optimizer.zero_grad()\n",
    "#     loss = 0\n",
    "#     for board_state, action, reward in zip(board_states, actions, rewards):\n",
    "#         output = model(board_state)\n",
    "#         row, col = action\n",
    "#         action_index = row * model.board_size + col\n",
    "#         target = torch.zeros_like(output).squeeze(0).flatten()\n",
    "#         target[action_index] = reward\n",
    "#         loss += ((output.squeeze(0).flatten() - target) ** 2).sum()\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "\n",
    "#白棋\n",
    "def play_with_gambling(model, board_size, learning_rate, num_games):\n",
    "    chess_board = []\n",
    "    for i in range(20):\n",
    "        t = []\n",
    "        for j in range(20):\n",
    "            t.append(0)\n",
    "        chess_board.append(t)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        board_state = torch.tensor(chess_board).reshape(1, 20, 20).float().to(device)\n",
    "        game_over = False\n",
    "        board_states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "\n",
    "        while not game_over:\n",
    "            x, y = place_where(chess_board)\n",
    "            board_states_list.append(board_state.clone())\n",
    "            actions_list.append([y, x])\n",
    "            board_state = update_board_state(board_state, [y, x], 1)\n",
    "            chess_board[y][x] = 1\n",
    "\n",
    "\n",
    "            #确定走法\n",
    "            move = select_move(model, board_state)\n",
    "            #将每一步棋走动时的棋盘记录下来，并避免使用引用\n",
    "            board_states_list.append(board_state.clone())\n",
    "            #记录每一步的走法\n",
    "            actions_list.append(move)            \n",
    "            board_state = update_board_state(board_state, move, 2)\n",
    "            chess_board[move[0]][move[1]] = 2\n",
    "\n",
    "\n",
    "            game_over, result = is_game_over(board_state, board_size)\n",
    "\n",
    "            #一局结束分配奖励\n",
    "            if game_over:\n",
    "                reward = calculate_reward(result, 2, 3)\n",
    "                rewards_list.append(reward)\n",
    "                if result == 2:\n",
    "                    update_model(model, optimizer, board_states_list, actions_list, rewards_list)\n",
    "                    print(\"博弈输\")\n",
    "                else:\n",
    "                    print(\"博弈胜\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def self_play(model_black, model_white, board_size, num_games, learning_rate):\n",
    "    # chess_board = []\n",
    "    # for i in range(20):\n",
    "    #     t = []\n",
    "    #     for j in range(20):\n",
    "    #         t.append(0)\n",
    "    #     chess_board.append(t)\n",
    "    \"\"\"\n",
    "    执行自我博弈过程并更新模型\n",
    "    model_black: 代表黑棋的模型\n",
    "    model_white: 代表白棋的模型\n",
    "    board_size: 棋盘大小\n",
    "    num_games: 自我博弈的局数\n",
    "    learning_rate: 学习率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #创建 Adam 优化器\n",
    "    #将模型中所有可学习的参数传递给优化器\n",
    "    optimizer_black = optim.Adam(model_black.parameters(), lr=learning_rate)\n",
    "    optimizer_white = optim.Adam(model_white.parameters(), lr=learning_rate)\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        board_state = torch.zeros(1, 20, 20).float().to(device)\n",
    "        game_over = False\n",
    "        current_color = 1  # 黑棋先下，用1表示\n",
    "        board_states_list = []\n",
    "        actions_list = []\n",
    "        rewards_list = []\n",
    "\n",
    "        while not game_over:\n",
    "\n",
    "            #选择下旗方\n",
    "            if current_color == 1:\n",
    "                model = model_black\n",
    "            else:\n",
    "                model = model_white\n",
    "\n",
    "            #确定走法\n",
    "            move = select_move(model, board_state)\n",
    "            #将每一步棋走动时的棋盘记录下来，并避免使用引用\n",
    "            board_states_list.append(board_state.clone())\n",
    "            #记录每一步的走法\n",
    "            actions_list.append(move)\n",
    "\n",
    "            board_state = update_board_state(board_state, move, current_color)\n",
    "            game_over, result = is_game_over(board_state, board_size)\n",
    "\n",
    "\n",
    "            #一局结束分配奖励\n",
    "            if game_over:\n",
    "                reward = calculate_reward(result, current_color, 1)\n",
    "                rewards_list.append(reward)\n",
    "                if result == 1:\n",
    "                    update_model(model_black, optimizer_black, board_states_list, actions_list, rewards_list)\n",
    "                    print(\"黑胜\")\n",
    "                elif result == 2:\n",
    "                    update_model(model_white, optimizer_white, board_states_list, actions_list, rewards_list)\n",
    "                    print(\"白胜\")\n",
    "            else:\n",
    "                current_color = 3 - current_color  # 切换棋子颜色\n",
    "\n",
    "    return model_black, model_white\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    board_size = 20\n",
    "    num_games = 200\n",
    "    learning_rate = 0.02\n",
    "    #每十轮与博弈算法进行一次对弈\n",
    "    model_black = GoBang_Model(board_size).to(device)\n",
    "    model_white = GoBang_Model(board_size).to(device)\n",
    "  \n",
    "    i = 1\n",
    "    while i <= num_games:\n",
    "        print(i)\n",
    "        if i % 10 == 0:\n",
    "            play_with_gambling(model_white, board_size, learning_rate, 1)\n",
    "            i += 1\n",
    "        else:\n",
    "            model_black, model_white = self_play(model_black, model_white, board_size, 9, learning_rate)\n",
    "            i += 9\n",
    "    model_black = model_black.to(device = 'cpu')\n",
    "    model_white = model_white.to(device = 'cpu')\n",
    "    torch.save(model_black, 'model_black_full.pth')\n",
    "    torch.save(model_white, 'model_white_full.pth')\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
